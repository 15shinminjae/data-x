{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-X Spring 2018\n",
    "## Tutorial on Regularization, Ridge, and Lasso Regression \n",
    "Reference:  \n",
    "* AARSHAY JAIN - A Complete Tutorial on Ridge and Lasso Regression in Python\n",
    "* https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Ridge and Lasso Regression are regularization techniques generally used for creating parsimonious models (Occam's Razor) in presence of a large number of features. The main goal is to combat tendency of models to overfit and balance the statistical-computational tradeoffs that are ubiquitous in the high-dimensional statistics era. \n",
    "\n",
    "* Ridge Regression\n",
    "    - L2-regularization \n",
    "    - Obj = Loss Function + $\\alpha\\|\\beta\\|_2$\n",
    "* Lasso Regression\n",
    "    - L1-regularization\n",
    "    - Obj = Loss Function + $\\alpha\\|\\beta\\|_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why penalize the magnitude of coefficients?\n",
    "\n",
    "Consider the following simulation of a sine curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries. The same will be used throughout the article.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 14, 10\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Define input array with angles from 60deg to 300deg converted to radians\n",
    "x_full = np.array([i*np.pi/180 for i in range(60,300,2)])\n",
    "np.random.seed(10)  #Setting seed for reproducability\n",
    "y_full = np.sin(x_full) + np.random.normal(0,0.15,len(x_full)) # Adding Noise\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(x_full,y_full,test_size=0.25)\n",
    "\n",
    "data = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])\n",
    "data_test = pd.DataFrame(np.column_stack([x_test,y_test]),columns=['x','y'])\n",
    "plt.plot(data['x'],data['y'],'.',ms=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = [data, data_test]\n",
    "\n",
    "for df in combine:\n",
    "    df.sort_values('x',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Thoughts: Polynomial Regression \n",
    "Features: $X^1, X^2, \\dots, X^{15}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in combine:\n",
    "\n",
    "    for i in range(2,16):  #power of 1 is already there\n",
    "        colname = 'x^%d'%i      #new var will be x^power\n",
    "        df[colname] = df['x']**i\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider building 15 models as follows:\n",
    "* Model 1: Features - $X^1$\n",
    "* Model 2: Features - $X, X^2$\n",
    "* $\\dots$\n",
    "* Model 15: Features - $X, X^2, X^{15}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Linear Regression model from scikit-learn.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "def linear_regression(data, power, models_to_plot):\n",
    "    #initialize predictors:\n",
    "    predictors=['x']\n",
    "    if power>=2:\n",
    "        predictors.extend(['x^%d'%i for i in range(2,power+1)])\n",
    "    \n",
    "    #Fit the model\n",
    "    linreg = LinearRegression(normalize=True)\n",
    "    linreg.fit(data[predictors],data['y'])\n",
    "    y_pred = linreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered power\n",
    "    if power in models_to_plot:\n",
    "        plt.subplot(models_to_plot[power])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.',ms=10)\n",
    "        plt.title('Plot for power: %d'%power)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    \n",
    "    y_pred_test = linreg.predict(data_test[predictors])\n",
    "    rss_test = sum((y_pred_test-data_test['y'])**2)\n",
    "    ret.extend([rss_test])\n",
    "    \n",
    "    ret.extend([linreg.intercept_])\n",
    "    ret.extend(linreg.coef_)\n",
    "    \n",
    "\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a dataframe to store the results:\n",
    "col = ['rss_train','rss_test','intercept'] + ['coef_x^%d'%i for i in range(1,16)]\n",
    "ind = ['model_pow_%d'%i for i in range(1,16)]\n",
    "coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the powers for which a plot is required:\n",
    "models_to_plot = {1:231,3:232,6:233,9:234,12:235,15:236}\n",
    "\n",
    "#Iterate through all powers and assimilate results\n",
    "for i in range(1,16):\n",
    "    coef_matrix_simple.iloc[i-1,0:i+3] = \\\n",
    "    linear_regression(data, power=i, models_to_plot=models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly aligns with our initial understanding. As the model complexity increases, the models tends to fit even smaller deviations in the training data set. We start to fit the gaussian noise in the data (patterns that is not there)\n",
    "\n",
    "However, as we add more features, we expect ourselves to overfit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly evident that the size of coefficients increase exponentially with increase in model complexity.\n",
    "\n",
    "What does a large coefficient signify? \n",
    "\n",
    "It means that we’re putting a lot of emphasis on that feature, i.e. the particular feature is a good predictor for the outcome. When it becomes too large, the algorithm starts modelling intricate relations to estimate the output and ends up overfitting to the particular training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Note the ‘Ridge’ function used here. It takes ‘alpha’ as a parameter on initialization. Also, keep in mind that normalizing the inputs is generally a good idea in every type of regression and should be used in case of ridge regression as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "def ridge_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    ridgereg = Ridge(alpha=alpha,normalize=True)\n",
    "    ridgereg.fit(data[predictors],data['y'])\n",
    "    y_pred = ridgereg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    \n",
    "    y_pred_test = ridgereg.predict(data_test[predictors])\n",
    "    rss_test = sum((y_pred_test-data_test['y'])**2)\n",
    "    ret.extend([rss_test])\n",
    "    \n",
    "    ret.extend([ridgereg.intercept_])\n",
    "    ret.extend(ridgereg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets analyze the result of Ridge regression for 10 different values of α ranging from 1e-15 to 20. Note that each of these 10 models will contain all the 15 variables and only the value of alpha would differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize predictors to be set of 15 powers of x\n",
    "predictors=['x']\n",
    "predictors.extend(['x^%d'%i for i in range(2,16)])\n",
    "\n",
    "#Set the different values of alpha to be tested\n",
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "\n",
    "#Initialize the dataframe for storing coefficients.\n",
    "col = ['rss_train','rss_test','intercept'] + ['coef_x^%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0,10)]\n",
    "coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n",
    "for i in range(10):\n",
    "    coef_matrix_ridge.iloc[i,] = ridge_regression(data, predictors, alpha_ridge[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly observe that as the value of alpha increases, the model complexity reduces. Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well (eg. alpha = 5). Thus alpha should be chosen wisely. A widely accept technique is cross-validation, i.e. the value of alpha is iterated over a range of values and the one giving higher cross-validation score is chosen.\n",
    "\n",
    "**USE CROSS VALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "This straight away gives us the following inferences:\n",
    "\n",
    "1. The RSS_train increases with increase in alpha, while rss_test is optimized when alpha is around 0.001 (note edge cases not perfect)\n",
    "2. An alpha as small as 1e-15 gives us significant reduction in magnitude of coefficients. How? Compare the coefficients in the first row of this table to the last row of simple linear regression table.\n",
    "3. High alpha values can lead to significant underfitting. Note the rapid increase in RSS for values of alpha greater than 1\n",
    "4. Though the coefficients are **very very small, they are NOT zero**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many of the coefficents are zero (in the full coefficient matrix)\n",
    "\n",
    "coef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that none of the coefficients are 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression \n",
    "#### (Least Absolute Shrinkage and Selection Operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "def lasso_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)\n",
    "    lassoreg.fit(data[predictors],data['y'])\n",
    "    y_pred = lassoreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    \n",
    "    y_pred_test = lassoreg.predict(data_test[predictors])\n",
    "    rss_test = sum((y_pred_test-data_test['y'])**2)\n",
    "    ret.extend([rss_test])\n",
    "    \n",
    "    ret.extend([lassoreg.intercept_])\n",
    "    ret.extend(lassoreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize predictors to all 15 powers of x\n",
    "predictors=['x']\n",
    "predictors.extend(['x^%d'%i for i in range(2,16)])\n",
    "\n",
    "#Define the alpha values to test\n",
    "alpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]\n",
    "\n",
    "#Initialize the dataframe to store coefficients\n",
    "col = ['rss_train','rss_test','intercept'] + ['coef_x^%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0,10)]\n",
    "coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the models to plot\n",
    "models_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 1:236}\n",
    "\n",
    "#Iterate over the 10 alpha values:\n",
    "for i in range(10):\n",
    "    coef_matrix_lasso.iloc[i,] = lasso_regression(data, predictors, alpha_lasso[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have high sparsity whenever a model coefficicent is zero.\n",
    "\n",
    "Seems to be best around alpha 10^-5 or 10^-4.\n",
    "\n",
    "### Inference:\n",
    "\n",
    "Apart from the expected inference of higher RSS for higher alphas, we can see the following:\n",
    "\n",
    "1. For the same values of alpha, the coefficients of lasso regression are much smaller as compared to that of ridge regression (compare row 1 of the 2 tables).\n",
    "2. For the same alpha, lasso has higher RSS (poorer fit) as compared to ridge regression\n",
    "3. Many of the coefficients are zero even for very small values of alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as $\\alpha$ increases, the number of coefficients being set to zero increases from 0 to 15. Implicitly, Lasso conducts feature selection as well!\n",
    "\n",
    "- Lasso works the best with sparsity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* Key Differences:\n",
    "    - Ridge: Typically includes all (or none) of the features in the model. \n",
    "        - Main advantage: Coefficient Shrinkage & Reducing Model Complexity\n",
    "        - It generally works well even in presence of highly correlated features as it will include all of them in the model but the coefficients will be distributed among them depending on the correlation.\n",
    "        - Prevents overfitting but does not reduce computational challenges\n",
    "        - simple gradient descent is capable of the optimization\n",
    "    - Lasso: \n",
    "        - Main advantage: Reduces model complexity & performs coefficient shrinking as well as feature selection\n",
    "        - Provides sparse solutions\n",
    "        - requires subgradient optimization in gradient descent\n",
    "\n",
    "* You can combine Lasso and Ridge Regression via Elastic Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration: Regularizing with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge with stochastic gradient descent\n",
    "iterations = [1,5,50,500,5e3,1e4]\n",
    "plt.subplots(2,3,figsize=(20,14))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    its = iterations[i]\n",
    "    \n",
    "    model = Ridge(alpha=0, normalize=True,max_iter=its,solver='sag',tol=1e-9)\n",
    "    model.fit(data[predictors], data['y'])\n",
    "    y_pred = model.predict(data[predictors])\n",
    "    plt.plot(data['x'],data['y'],'.',ms=16)\n",
    "    plt.plot(data['x'],y_pred)\n",
    "    plt.title('Max Iterations {}'.format(its))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
